Rank,Model,Accuracy,Dev Accuracy,Paper Title,Paper URL
1,T5-11B,97.5,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning
2,MT-DNN-SMART,97.5,,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
3,T5-3B,97.4,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning
4,MUPPET Roberta Large,97.4,,Muppet: Massive Multi-task Representations with Pre-Finetuning,/paper/muppet-massive-multi-task-representations
5,ALBERT,97.1,,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,/paper/albert-a-lite-bert-for-self-supervised
6,StructBERTRoBERTa ensemble,97.1,,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,/paper/structbert-incorporating-language-structures
7,XLNet ,97.0,,XLNet: Generalized Autoregressive Pretraining for Language Understanding,/paper/xlnet-generalized-autoregressive-pretraining
8,ELECTRA,96.9,,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,/paper/electra-pre-training-text-encoders-as-1
9,RoBERTa-large 355M + Entailment as Few-shot Learner,96.9,,Entailment as Few-Shot Learner,/paper/entailment-as-few-shot-learner
10,XLNet-Large ,96.8,,XLNet: Generalized Autoregressive Pretraining for Language Understanding,/paper/xlnet-generalized-autoregressive-pretraining
11,FLOATER-large,96.7,,Learning to Encode Position for Transformer with Continuous Dynamical Model,/paper/learning-to-encode-position-for-transformer
12,MUPPET Roberta base,96.7,,Muppet: Massive Multi-task Representations with Pre-Finetuning,/paper/muppet-massive-multi-task-representations
13,RoBERTa ,96.7,,RoBERTa: A Robustly Optimized BERT Pretraining Approach,/paper/roberta-a-robustly-optimized-bert-pretraining
14,DeBERTa ,96.5,,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,/paper/deberta-decoding-enhanced-bert-with
15,MT-DNN-ensemble,96.5,,Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding,/paper/improving-multi-task-deep-neural-networks-via
16,RoBERTa-large 355M ,96.4,,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,/paper/llm-int8-8-bit-matrix-multiplication-for
17,ASA + RoBERTa,96.3,,Adversarial Self-Attention for Language Understanding,/paper/adversarial-self-attention-for-language
18,T5-Large 770M,96.3,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning
19,Snorkel MeTaL,96.2,,Training Complex Models with Multi-Task Weak Supervision,/paper/training-complex-models-with-multi-task-weak
20,PSQ ,96.2,,A Statistical Framework for Low-bitwidth Training of Deep Neural Networks,/paper/a-statistical-framework-for-low-bitwidth
21,Heinsen Routing + RoBERTa-large,96.0,,An Algorithm for Routing Vectors in Sequences,/paper/an-algorithm-for-routing-vectors-in-sequences
22,MT-DNN,95.6,,Multi-Task Deep Neural Networks for Natural Language Understanding,/paper/multi-task-deep-neural-networks-for-natural
23,Heinsen Routing + GPT-2,95.6,,An Algorithm for Routing Capsules in All Domains,/paper/an-algorithm-for-routing-capsules-in-all
24,T5-Base,95.2,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning
25,ERNIE 2.0 Base,95.0,,ERNIE 2.0: A Continual Pre-training Framework for Language Understanding,/paper/ernie-20-a-continual-pre-training-framework
26,RoBERTa+DualCL,94.91,,Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation,/paper/dual-contrastive-learning-text-classification
27,BERT-LARGE,94.9,,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,/paper/bert-pre-training-of-deep-bidirectional
28,SpanBERT,94.8,,SpanBERT: Improving Pre-training by Representing and Predicting Spans,/paper/spanbert-improving-pre-training-by
29,gMLP-large,94.8,,Pay Attention to MLPs,/paper/pay-attention-to-mlps
30,Q-BERT ,94.8,,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,/paper/q-bert-hessian-based-ultra-low-precision
31,Q8BERT ,94.7,,Q8BERT: Quantized 8Bit BERT,/paper/q8bert-quantized-8bit-bert
32,CNN Large,94.6,,Cloze-driven Pretraining of Self-attention Networks,/paper/cloze-driven-pretraining-of-self-attention
33,BigBird,94.6,,Big Bird: Transformers for Longer Sequences,/paper/big-bird-transformers-for-longer-sequences
34,MLM+ del-word+ reorder,94.5,,CLEAR: Contrastive Learning for Sentence Representation,/paper/clear-contrastive-learning-for-sentence
35,ASA + BERT-base,94.1,,Adversarial Self-Attention for Language Understanding,/paper/adversarial-self-attention-for-language
36,RealFormer,94.04,,RealFormer: Transformer Likes Residual Attention,/paper/informer-transformer-likes-informed-attention
37,FNet-Large,94.0,,FNet: Mixing Tokens with Fourier Transforms,/paper/fnet-mixing-tokens-with-fourier-transforms
38,MT-DNN,93.6,,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
39,ERNIE,93.5,,ERNIE: Enhanced Language Representation with Informative Entities,/paper/ernie-enhanced-language-representation-with
40,Block-sparse LSTM,93.2,,GPU Kernels for Block-Sparse Weights,/paper/gpu-kernels-for-block-sparse-weights
41,LM-CPPF RoBERTa-base,93.2,,LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning,/paper/lm-cppf-paraphrasing-guided-data-augmentation
42,BERT Large,93.1,,Fine-grained Sentiment Classification using BERT,/paper/fine-grained-sentiment-classification-using
43,TinyBERT-6 67M,93.1,,TinyBERT: Distilling BERT for Natural Language Understanding,/paper/190910351
44,24hBERT,93.0,,How to Train BERT with an Academic Budget,/paper/how-to-train-bert-with-an-academic-budget
45,SMART+BERT-BASE,93.0,,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
46,TinyBERT-4 14.5M,92.6,,TinyBERT: Distilling BERT for Natural Language Understanding,/paper/190910351
47,bmLSTM,91.8,,Learning to Generate Reviews and Discovering Sentiment,/paper/learning-to-generate-reviews-and-discovering
48,T5-Small,91.8,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning
49,byte mLSTM7,91.7,,A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors,/paper/a-la-carte-embedding-cheap-but-effective
50,PAR BERT Base,91.6,,Pay Attention when Required,/paper/pay-attention-when-required
51,Charformer-Base,91.6,,Charformer: Fast Character Transformers via Gradient-based Subword Tokenization,/paper/charformer-fast-character-transformers-via
52,SqueezeBERT,91.4,,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,/paper/squeezebert-what-can-computer-vision-teach
53,Nyströmformer,91.4,,Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention,/paper/nystromformer-a-nystrom-based-algorithm-for
54,Bi-CAS-LSTM,91.3,,Cell-aware Stacked LSTMs for Modeling Sentences,/paper/cell-aware-stacked-lstms-for-modeling
55,DistilBERT 66M,91.3,,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",/paper/distilbert-a-distilled-version-of-bert
56,CNN,91.2,,On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis,/paper/on-the-role-of-text-preprocessing-in-neural
57,Suffix BiLSTM,91.2,,Improved Sentence Modeling using Suffix Bidirectional LSTM,/paper/improved-sentence-modeling-using-suffix
58,BERT Base,91.2,,Fine-grained Sentiment Classification using BERT,/paper/fine-grained-sentiment-classification-using
59,Transformer ,90.9,,Practical Text Classification With Large Pre-Trained Language Models,/paper/practical-text-classification-with-large-pre
60,Single layer bilstm distilled from BERT,90.7,,Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,/paper/distilling-task-specific-knowledge-from-bert
61,BCN+Char+CoVe,90.3,,Learned in Translation: Contextualized Word Vectors,/paper/learned-in-translation-contextualized-word
62,CNN-RNF-LSTM,90.0,,Convolutional Neural Networks with Recurrent Neural Filters,/paper/convolutional-neural-networks-with-recurrent
63,Neural Semantic Encoder,89.7,,Neural Semantic Encoders,/paper/neural-semantic-encoders
64,BLSTM-2DCNN,89.5,,Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling,/paper/text-classification-improved-by-integrating
65,CNN + Logic rules,89.3,,Harnessing Deep Neural Networks with Logic Rules,/paper/harnessing-deep-neural-networks-with-logic
66,DMN [ankit16],88.6,,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,/paper/ask-me-anything-dynamic-memory-networks-for
67,CNN-MC [kim:13],88.1,,Convolutional Neural Networks for Sentence Classification,/paper/convolutional-neural-networks-for-sentence
68,CT-LSTM[tai2015improved],88.0,,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,/paper/improved-semantic-representations-from-tree
69,C-LSTM,87.8,,A C-LSTM Neural Network for Text Classification,/paper/a-c-lstm-neural-network-for-text
70,MPAD-path,87.75,,Message Passing Attention Networks for Document Understanding,/paper/message-passing-attention-networks-for
71,Standard DR-AGG,87.6,,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing
72,USE_T+CNN ,87.21,,Universal Sentence Encoder,/paper/universal-sentence-encoder
73,Reverse DR-AGG,87.2,,Information Aggregation via Dynamic Routing for Sequence Encoding,/paper/information-aggregation-via-dynamic-routing
74,DC-MCNN,86.99,,A Helping Hand: Transfer Learning for Deep Sentiment Analysis,/paper/a-helping-hand-transfer-learning-for-deep
75,STM+TSED+PT+2L,86.95,,The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning,/paper/190600095
76,Capsule-B ,86.8,,Investigating Capsule Networks with Dynamic Routing for Text Classification,/paper/investigating-capsule-networks-with-dynamic
77,2-layer LSTM[tai2015improved],86.3,,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,/paper/improved-semantic-representations-from-tree
78,RNTN,85.4,,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,/paper/recursive-deep-models-for-semantic
79,SWEM-concat,84.3,,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,/paper/baseline-needs-more-love-on-simple-word
80,MV-RNN,82.9,,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,/paper/recursive-deep-models-for-semantic
81,GloVe+Emo2Vec,82.3,,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion
82,Emo2Vec,81.2,,Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training,/paper/emo2vec-learning-generalized-emotion
83,ToWE-CBOW,78.8,,Task-oriented Word Embedding for Text Classification,/paper/task-oriented-word-embedding-for-text
84,Joined Model Multi-tasking,54.72,,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,/paper/exploring-joint-neural-model-for-sentence
85,SMARTRoBERTa,,96.9,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
86,SMART-MT-DNN,,96.1,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
87,SMART-BERT,,93.0,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,/paper/smart-robust-and-efficient-fine-tuning-for
