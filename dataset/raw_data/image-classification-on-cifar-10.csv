Rank,Model,Percentage correct,PARAMS,Paper Title,Paper URL
1,efficient adaptive ensembling,99.612,,Efficient Adaptive Ensembling for Image Classification,/paper/efficient-adaptive-ensembling-for-image
2,ViT-H/14,99.5,632000000.0,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,/paper/an-image-is-worth-16x16-words-transformers-1
3,DINOv2 ,99.5,1100000000.0,DINOv2: Learning Robust Visual Features without Supervision,/paper/dinov2-learning-robust-visual-features
4,Âµ2Net ,99.49,,An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems,/paper/an-evolutionary-approach-to-dynamic
5,ViT-L/16,99.42,,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,/paper/an-image-is-worth-16x16-words-transformers-1
6,CaiT-M-36 U 224,99.4,,Going deeper with Image Transformers,/paper/going-deeper-with-image-transformers
7,CvT-W24,99.39,,CvT: Introducing Convolutions to Vision Transformers,/paper/cvt-introducing-convolutions-to-vision
8,BiT-L ,99.37,,Big Transfer (BiT): General Visual Representation Learning,/paper/large-scale-learning-of-general-visual
9,RDNet-L ,99.31,186000000.0,DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs,/paper/densenets-reloaded-paradigm-shift-beyond
10,ViT-B ,99.3,,Three things everyone should know about Vision Transformers,/paper/three-things-everyone-should-know-about
11,RDNet-B ,99.28,87000000.0,DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs,/paper/densenets-reloaded-paradigm-shift-beyond
12,Heinsen Routing + BEiT-large 16 224,99.2,309500000.0,An Algorithm for Routing Vectors in Sequences,/paper/an-algorithm-for-routing-vectors-in-sequences
13,ViT-B/16 ,99.13,,Perturbated Gradients Updating within Unit Space for Deep Learning,/paper/update-in-unit-gradient
14,Astroformer,99.12,,Astroformer: More Data Might not be all you need for Classification,/paper/astroformer-more-data-might-not-be-all-you
15,CeiT-S ,99.1,,Incorporating Convolution Designs into Visual Transformers,/paper/incorporating-convolution-designs-into-visual
16,AutoFormer-S | 384,99.1,23000000.0,AutoFormer: Searching Transformers for Visual Recognition,/paper/autoformer-searching-transformers-for-visual
17,TNT-B,99.1,65600000.0,Transformer in Transformer,/paper/transformer-in-transformer
18,DeiT-B,99.1,86000000.0,Training data-efficient image transformers & distillation through attention,/paper/training-data-efficient-image-transformers
19,EfficientNetV2-L,99.1,121000000.0,EfficientNetV2: Smaller Models and Faster Training,/paper/efficientnetv2-smaller-models-and-faster
20,VIT-L/16,99.05,,Reduction of Class Activation Uncertainty with Background Information,/paper/reduction-of-class-activation-uncertainty
21,LaNet,99.03,44100000.0,Sample-Efficient Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search,/paper/sample-efficient-neural-architecture-search-1
22,RDNet-S ,99.01,50000000.0,DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs,/paper/densenets-reloaded-paradigm-shift-beyond
23,GPIPE + transfer learning,99.0,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,/paper/gpipe-efficient-training-of-giant-neural
24,TResNet-XL,99.0,,TResNet: High Performance GPU-Dedicated Architecture,/paper/tresnet-high-performance-gpu-dedicated
25,CeiT-S,99.0,,Incorporating Convolution Designs into Visual Transformers,/paper/incorporating-convolution-designs-into-visual
26,GFNet-H-B,99.0,54000000.0,Global Filter Networks for Image Classification,/paper/global-filter-networks-for-image
27,EfficientNetV2-M,99.0,55000000.0,EfficientNetV2: Smaller Models and Faster Training,/paper/efficientnetv2-smaller-models-and-faster
28,BiT-M ,98.91,,Big Transfer (BiT): General Visual Representation Learning,/paper/large-scale-learning-of-general-visual
29,EfficientNet-B7,98.9,64000000.0,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,/paper/efficientnet-rethinking-model-scaling-for
30,RDNet-T ,98.88,24000000.0,DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs,/paper/densenets-reloaded-paradigm-shift-beyond
31,ASF-former-B,98.8,,Adaptive Split-Fusion Transformer,/paper/adaptive-split-fusion-transformer
32,"PyramidNet-272, S=4",98.71,32600000.0,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,/paper/splitnet-divide-and-co-training
33,ASF-former-S,98.7,,Adaptive Split-Fusion Transformer,/paper/adaptive-split-fusion-transformer
34,EfficientNetV2-S,98.7,24000000.0,EfficientNetV2: Smaller Models and Faster Training,/paper/efficientnetv2-smaller-models-and-faster
35,PyramidNet-272 ,98.68,26000000.0,ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks,/paper/asam-adaptive-sharpness-aware-minimization
36,PyramidNet + ShakeDrop + Fast AA + FMix,98.64,26210000.0,FMix: Enhancing Mixed Sample Data Augmentation,/paper/understanding-and-enhancing-mixed-sample-data
37,ConvMLP-M,98.6,,ConvMLP: Hierarchical Convolutional MLPs for Vision,/paper/convmlp-hierarchical-convolutional-mlps-for
38,ConvMLP-L,98.6,,ConvMLP: Hierarchical Convolutional MLPs for Vision,/paper/convmlp-hierarchical-convolutional-mlps-for
39,ViT-B/16- SAM,98.6,87000000.0,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
40,DVT ,98.53,,Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition,/paper/not-all-images-are-worth-16x16-words-dynamic
41,E2E-3M,98.52,20000000.0,Rethinking Recurrent Neural Networks and Other Improvements for Image Classification,/paper/rethinking-recurrent-neural-networks-and
42,CeiT-T,98.5,,Incorporating Convolution Designs into Visual Transformers,/paper/incorporating-convolution-designs-into-visual
43,NAT-M4,98.4,6900000.0,Neural Architecture Transfer,/paper/neural-architecture-transfer
44,"WRN-40-10, S=4",98.38,55900000.0,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,/paper/splitnet-divide-and-co-training
45,"WRN-28-10, S=4",98.32,36700000.0,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,/paper/splitnet-divide-and-co-training
46,Dynamics 2,98.31,,PSO-Convolutional Neural Networks with Heterogeneous Learning Rate,/paper/pso-convolutional-neural-networks-with
47,"Shake-Shake 26 2x96d, S=4",98.31,26300000.0,Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training,/paper/splitnet-divide-and-co-training
48,ResNet50 ,98.3,25000000.0,ResNet strikes back: An improved training procedure in timm,/paper/resnet-strikes-back-an-improved-training
49,PyramidNet+ShakeDrop ,98.3,26210000.0,Fast AutoAugment,/paper/fast-autoaugment
50,NoisyDARTS-A-t,98.28,,Noisy Differentiable Architecture Search,/paper/noisy-differentiable-architecture-search
51,LeViT-192,98.2,,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,/paper/levit-a-vision-transformer-in-convnet-s
52,ResNet-152-SAM,98.2,,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
53,ViT-S/16- SAM,98.2,,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
54,Bamboo ,98.2,,Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy,/paper/bamboo-building-mega-scale-vision-dataset
55,NAT-M3,98.2,6200000.0,Neural Architecture Transfer,/paper/neural-architecture-transfer
56,LeViT-256,98.1,,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,/paper/levit-a-vision-transformer-in-convnet-s
57,PyramidNet + AA ,98.02,27220000.0,Regularizing Neural Networks via Adversarial Model Perturbation,/paper/regularizing-neural-networks-via-adversarial
58,EnAET,98.01,36500000.0,EnAET: A Self-Trained framework for Semi-Supervised and Supervised Learning with Ensemble Transformations,/paper/enaet-self-trained-ensemble-autoencoding
59,LeViT-384,98.0,,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,/paper/levit-a-vision-transformer-in-convnet-s
60,ConvMLP-S,98.0,,ConvMLP: Hierarchical Convolutional MLPs for Vision,/paper/convmlp-hierarchical-convolutional-mlps-for
61,MUXNet-m,98.0,2100000.0,MUXConv: Information Multiplexing in Convolutional Neural Networks,/paper/muxconv-information-multiplexing-in
62,CCT-7/3x1*,98.0,3760000.0,Escaping the Big Data Paradigm with Compact Transformers,/paper/escaping-the-big-data-paradigm-with-compact
63,Proxyless-G + c/o,97.92,5700000.0,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,/paper/proxylessnas-direct-neural-architecture
64,NAT-M2,97.9,4600000.0,Neural Architecture Transfer,/paper/neural-architecture-transfer
65,WRN-28-10+AutoDropout+RandAugment,97.9,36500000.0,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,/paper/autodropout-learning-dropout-patterns-to
66,SENet + ShakeShake + Cutout,97.88,,Squeeze-and-Excitation Networks,/paper/squeeze-and-excitation-networks
67,HCGNet-A3,97.86,11400000.0,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,/paper/gated-convolutional-networks-with-hybrid
68,Wide-ResNet-28-10,97.85,,Automatic Data Augmentation via Invariance-Constrained Learning,/paper/automatic-data-augmentation-via-invariance
69,ResNeXt-50 ,97.84,,AutoMix: Unveiling the Power of Mixup for Stronger Classifiers,/paper/automix-unveiling-the-power-of-mixup
70,ResNet-152x4-AGC ,97.82,,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,/paper/effect-of-large-scale-pre-training-on-full
71,Mixer-B/16- SAM,97.8,,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
72,CCT-7/3x1+VTM,97.78,,TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers,/paper/tokenmixup-efficient-attention-guided-token
73,WRN-28-10,97.73,36500000.0,MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks,/paper/mixmo-mixing-multiple-inputs-for-multiple
74,HCGNet-A2,97.71,3100000.0,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,/paper/gated-convolutional-networks-with-hybrid
75,WRN + fixup init + mixup + cutout,97.7,18000000.0,Fixup Initialization: Residual Learning Without Normalization,/paper/fixup-initialization-residual-learning
76,TransBoost-ResNet50,97.61,,TransBoost: Improving the Best ImageNet Performance using Deep Transduction,/paper/transboost-improving-the-best-imagenet
77,NoisyDARTS-a,97.61,5500000.0,Noisy Differentiable Architecture Search,/paper/noisy-differentiable-architecture-search
78,LeViT-128,97.6,,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,/paper/levit-a-vision-transformer-in-convnet-s
79,DenseNet-BC-190 + batchboost,97.54,25600000.0,batchboost: regularization for stabilizing training with resistance to underfitting & overfitting,/paper/batchboost-regularization-for-stabilizing
80,LeViT-128S,97.5,,LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference,/paper/levit-a-vision-transformer-in-convnet-s
81,Shared WRN,97.47,33500000.0,Learning Implicitly Recurrent CNNs Through Parameter Sharing,/paper/learning-implicitly-recurrent-cnns-through
82,Manifold Mixup WRN 28-10,97.45,36500000.0,Manifold Mixup: Better Representations by Interpolating Hidden States,/paper/manifold-mixup-better-representations-by
83,WRN 28-14,97.45,36500000.0,Neural networks with late-phase weights,/paper/economical-ensembles-with-hypernetworks
84,SparseSwin,97.43,17580000.0,SparseSwin: Swin Transformer with Sparse Transformer Block,/paper/sparseswin-swin-transformer-with-sparse
85,WRN-28-10 with reSGHMC,97.42,36500000.0,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,/paper/non-convex-learning-via-replica-exchange
86,NAT-M1,97.4,4300000.0,Neural Architecture Transfer,/paper/neural-architecture-transfer
87,ResNet-50-SAM,97.4,25000000.0,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
88,kNN-CLIP,97.3,,Revisiting a kNN-based Image Classification System with High-capacity Storage,/paper/revisiting-a-knn-based-image-classification
89,DenseNet-BC-190 + Mixup,97.3,25600000.0,mixup: Beyond Empirical Risk Minimization,/paper/mixup-beyond-empirical-risk-minimization
90,WaveMixLite-144/7,97.29,,WaveMix: A Resource-efficient Neural Network for Image Analysis,/paper/wavemix-lite-a-resource-efficient-neural
91,Transformer local-attention ,97.2,90100000.0,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",/paper/aggregating-nested-transformers
92,ShakeShake-2x64d + SWA,97.12,,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and
93,PyramidNet-200 + CutMix,97.12,,CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features,/paper/cutmix-regularization-strategy-to-train
94,Wide-ResNet-40-2,97.05,,Automatic Data Augmentation via Invariance-Constrained Learning,/paper/automatic-data-augmentation-via-invariance
95,ORN,97.02,,Oriented Response Networks,/paper/oriented-response-networks
96,WRN-16-8 with reSGHMC,96.87,,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,/paper/non-convex-learning-via-replica-exchange
97,ResNet_XnIDR,96.87,,XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks,/paper/xnodr-and-xnidr-two-accurate-and-fast-fully
98,HCGNet-A1,96.85,1100000.0,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,/paper/gated-convolutional-networks-with-hybrid
99,WRN 28-10,96.81,,Neural networks with late-phase weights,/paper/economical-ensembles-with-hypernetworks
100,AutoDropout,96.8,,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,/paper/autodropout-learning-dropout-patterns-to
101,WRN-28-10 + SWA,96.79,,Averaging Weights Leads to Wider Optima and Better Generalization,/paper/averaging-weights-leads-to-wider-optima-and
102,ConvMixer-256/16,96.74,1340000.0,Patches Are All You Need?,/paper/patches-are-all-you-need-1
103,EXACT ,96.73,,EXACT: How to Train Your Accuracy,/paper/exact-how-to-train-your-accuracy
104,Wide ResNet+cutout,96.71,,Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems,/paper/single-bit-per-weight-deep-convolutional
105,Deep pyramidal residual network,96.69,,Deep Pyramidal Residual Networks,/paper/deep-pyramidal-residual-networks
106,CoPaNet-R-164,96.62,,Deep Competitive Pathway Networks,/paper/deep-competitive-pathway-networks
107,DenseNet ,96.54,,Densely Connected Convolutional Networks,/paper/densely-connected-convolutional-networks
108,SKNet-29 ,96.53,,Selective Kernel Networks,/paper/selective-kernel-networks
109,Fractional MP,96.5,,Fractional Max-Pooling,/paper/fractional-max-pooling
110,PDO-eConv ,96.5,,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,/paper/pdo-econvs-partial-differential-operator
111,UPANets,96.47,,UPANets: Learning from the Universal Pixel Attention Networks,/paper/upanets-learning-from-the-universal-pixel
112,GAC-SNN,96.46,,Gated Attention Coding for Training High-performance and Efficient Spiking Neural Networks,/paper/gated-attention-coding-for-training-high
113,ViT ,96.41,3640000.0,Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images,/paper/pre-training-of-lightweight-vision
114,NAS-RL,96.4,,Neural Architecture Search with Reinforcement Learning,/paper/neural-architecture-search-with-reinforcement
115,VGG11B,96.4,,Training Neural Networks with Local Error Signals,/paper/training-neural-networks-with-local-error
116,Residual Gates + WRN,96.35,,Learning Identity Mappings with Residual Gates,/paper/learning-identity-mappings-with-residual
117,PDO-eConv ,96.32,,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,/paper/pdo-econvs-partial-differential-operator
118,SimpleNetv2,96.29,,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,/paper/towards-principled-design-of-deep
119,ResNet56 with reSGHMC,96.12,,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,/paper/non-convex-learning-via-replica-exchange
120,Mixer-S/16- SAM,96.1,,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,/paper/when-vision-transformers-outperform-resnets
121,PreActResNet18 ,96.03,,Regularizing Neural Networks via Adversarial Model Perturbation,/paper/regularizing-neural-networks-via-adversarial
122,ConvMixer-256/8,96.03,710000.0,Patches Are All You Need?,/paper/patches-are-all-you-need-1
123,Local Mixup Resnet18,95.97,,Preventing Manifold Intrusion with Locality: Local Mixup,/paper/preventing-manifold-intrusion-with-locality
124,ResNet-50x1-ACG ,95.78,,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,/paper/effect-of-large-scale-pre-training-on-full
125,ACN,95.6,,Striving for Simplicity: The All Convolutional Net,/paper/striving-for-simplicity-the-all-convolutional
126,Evolution ensemble,95.6,,Large-Scale Evolution of Image Classifiers,/paper/large-scale-evolution-of-image-classifiers
127,ResNet-18,95.55,,"Benchopt: Reproducible, efficient and collaborative optimization benchmarks",/paper/benchopt-reproducible-efficient-and
128,SimpleNetv1,95.51,,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",/paper/lets-keep-it-simple-using-simple
129,IM-Loss ,95.49,,IM-Loss: Information Maximization Loss for Spiking Neural Networks,/paper/im-loss-information-maximization-loss-for
130,ResNet-1001,95.4,,Identity Mappings in Deep Residual Networks,/paper/identity-mappings-in-deep-residual-networks
131,ResNet32 with reSGHMC,95.35,,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,/paper/non-convex-learning-via-replica-exchange
132,ResNet-18+MM+FRL,95.33,,Learning Class Unique Features in Fine-Grained Visual Classification,/paper/towards-class-specific-unit
133,PSN ,95.32,,,
134,CCT-6/3x1,95.29,3170000.0,Escaping the Big Data Paradigm with Compact Transformers,/paper/escaping-the-big-data-paradigm-with-compact
135,MomentumNet,95.18,,Momentum Residual Neural Networks,/paper/momentum-residual-neural-networks
136,Context-Aware Pipeline,95.16,,Context-Aware Compilation of DNN Training Pipelines across Edge and Cloud,/paper/context-aware-compilation-of-dnn-training
137,SRM-ResNet-56,95.05,,SRM : A Style-based Recalibration Module for Convolutional Neural Networks,/paper/srm-a-style-based-recalibration-module-for
138,MixMatch,95.05,,MixMatch: A Holistic Approach to Semi-Supervised Learning,/paper/mixmatch-a-holistic-approach-to-semi
139,WRN-22-8 ,95.04,,Sparse Networks from Scratch: Faster Training without Losing Performance,/paper/sparse-networks-from-scratch-faster-training
140,LP-BNN ,95.02,,Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification,/paper/encoding-the-latent-posterior-of-bayesian
141,kEffNet-B0 V2 32ch + H Flip,94.95,950000.0,An Enhanced Scheme for Reducing the Complexity of Pointwise Convolutions in CNNs for Image Classification Based on Interleaved Grouped Filters without Divisibility Constraints,/paper/an-enhanced-scheme-for-reducing-the
142,Prodpoly,94.9,,Deep Polynomial Neural Networks,/paper/deep-polynomial-neural-networks
143,ResNet-9,94.79,,CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters,/paper/cnn-filter-db-an-empirical-investigation-of
144,Stochastic Depth,94.77,,Deep Networks with Stochastic Depth,/paper/deep-networks-with-stochastic-depth
145,VGG-19 with GradInit,94.71,20030000.0,GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training,/paper/gradinit-learning-to-initialize-neural
146,ResNet20 with reSGHMC,94.62,,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,/paper/non-convex-learning-via-replica-exchange
147,PDO-eConv ,94.62,,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,/paper/pdo-econvs-partial-differential-operator
148,Evolution,94.6,,Large-Scale Evolution of Image Classifiers,/paper/large-scale-evolution-of-image-classifiers
149,RL+NT,94.6,,Efficient Architecture Search by Network Transformation,/paper/efficient-architecture-search-by-network
150,Convolutional Performer for Vision ,94.46,1300000.0,Convolutional Xformers for Vision,/paper/convolutional-xformers-for-vision
151,ResNet+ELU,94.4,,Deep Residual Networks with Exponential Linear Unit,/paper/deep-residual-networks-with-exponential
152,Deep Complex,94.4,,Deep Complex Networks,/paper/deep-complex-networks
153,PDO-eConv ,94.35,,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,/paper/pdo-econvs-partial-differential-operator
154,Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,94.29,4300000.0,Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,/paper/stochastic-optimization-of-plain
155,Fitnet4-LSUV,94.2,,All you need is a good init,/paper/all-you-need-is-a-good-init
156,ResNet 9 + Mish,94.05,,Mish: A Self Regularized Non-Monotonic Activation Function,/paper/mish-a-self-regularized-non-monotonic-neural
157,Tree+Max-Avg pooling,94.0,,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",/paper/generalizing-pooling-functions-in
158,Beta-Rank,93.97,,Beta-Rank: A Robust Convolutional Filter Pruning Method For Imbalanced Medical Image Analysis,/paper/beta-rank-a-robust-convolutional-filter
159,SA quadratic embedding,93.8,,On the Relationship between Self-Attention and Convolutional Layers,/paper/on-the-relationship-between-self-attention-1
160,kEffNet-B0 32ch,93.75,1060000.0,Grouped Pointwise Convolutions Reduce Parameters in Convolutional Neural Networks,/paper/grouped-pointwise-convolutions-reduce
161,OTTT,93.73,,Online Training Through Time for Spiking Neural Networks,/paper/online-training-through-time-for-spiking
162,SSCNN,93.7,,Spatially-sparse convolutional neural networks,/paper/spatially-sparse-convolutional-neural
163,NNCLR,93.7,,With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations,/paper/with-a-little-help-from-my-friends-nearest
164,Tuned CNN,93.6,,Scalable Bayesian Optimization Using Deep Neural Networks,/paper/scalable-bayesian-optimization-using-deep
165,Exponential Linear Units,93.5,,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),/paper/fast-and-accurate-deep-network-learning-by
166,BNM NiN,93.3,,Batch-normalized Maxout Network in Network,/paper/batch-normalized-maxout-network-in-network
167,Universum Prescription,93.3,,Universum Prescription: Regularization using Unlabeled Data,/paper/universum-prescription-regularization-using
168,CMsC,93.1,,Competitive Multi-scale Convolution,/paper/competitive-multi-scale-convolution
169,DGPPF-ResNet18,92.9,570000.0,Distilled Gradual Pruning with Pruned Fine-tuning,/paper/distilled-gradual-pruning-with-pruned-fine
170,kMobileNet V3 Large 16ch,92.74,400000.0,Grouped Pointwise Convolutions Reduce Parameters in Convolutional Neural Networks,/paper/grouped-pointwise-convolutions-reduce
171,NiN+APL,92.5,,Learning Activation Functions to Improve Deep Neural Networks,/paper/learning-activation-functions-to-improve-deep
172,VDN,92.4,,Training Very Deep Networks,/paper/training-very-deep-networks
173,ResNet,92.3,,A Bregman Learning Framework for Sparse Neural Networks,/paper/a-bregman-learning-framework-for-sparse
174,SWWAE,92.2,,Stacked What-Where Auto-encoders,/paper/stacked-what-where-auto-encoders
175,FlexTCN-7,92.2,670000.0,FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes,/paper/flexconv-continuous-kernel-convolutions-with-1
176,ReActNet-18,92.08,,"""BNN - BN = ?"": Training Binary Neural Networks without Batch Normalization",/paper/bnn-bn-training-binary-neural-networks
177,ResNet v2-20 ,92.02,,Mish: A Self Regularized Non-Monotonic Activation Function,/paper/mish-a-self-regularized-non-monotonic-neural
178,Context-Aware DNN tree,92.01,,Context-aware deep model compression for edge cloud computing,/paper/context-aware-deep-model-compression-for-edge
179,DSN,91.8,,Deeply-Supervised Nets,/paper/deeply-supervised-nets
180,BinaryConnect,91.7,,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,/paper/binaryconnect-training-deep-neural-networks
181,CLS-GAN,91.7,,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,/paper/loss-sensitive-generative-adversarial
182,MIM,91.5,,On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units,/paper/on-the-importance-of-normalisation-layers-in
183,Spectral Representations for Convolutional Neural Networks,91.4,,Spectral Representations for Convolutional Neural Networks,/paper/spectral-representations-for-convolutional
184,DLME ,91.3,,DLME: Deep Local-flatness Manifold Embedding,/paper/dlme-deep-local-flatness-manifold-embedding
185,RMDL ,91.21,,RMDL: Random Multimodel Deep Learning for Classification,/paper/rmdl-random-multimodel-deep-learning-for
186,Network in Network,91.2,,Network In Network,/paper/network-in-network
187,ResNet-26 ,91.1,366000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
188,ResNet-32 ,90.9,464000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
189,kDenseNet-BC L100 12ch,90.83,350000.0,Grouped Pointwise Convolutions Reduce Parameters in Convolutional Neural Networks,/paper/grouped-pointwise-convolutions-reduce
190,Deep Networks with Internal Selective Attention through Feedback Connections,90.8,,Deep Networks with Internal Selective Attention through Feedback Connections,/paper/deep-networks-with-internal-selective
191,Maxout Network ,90.65,,Maxout Networks,/paper/maxout-networks
192,ResNet-18,90.65,,"Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation",/paper/knowledge-representing-efficient-sparse
193,DNN+Probabilistic Maxout,90.6,,Improving Deep Neural Networks with Probabilistic Maxout Units,/paper/improving-deep-neural-networks-with
194,GP EI,90.5,,Practical Bayesian Optimization of Machine Learning Algorithms,/paper/practical-bayesian-optimization-of-machine
195,ResNet-44 ,90.5,658000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
196,ResNet-20 ,90.4,269000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
197,SEER ,90.0,,Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision,/paper/vision-models-are-more-robust-and-fair-when
198,kMobileNet 16ch,89.81,240000.0,Grouped Pointwise Convolutions Reduce Parameters in Convolutional Neural Networks,/paper/grouped-pointwise-convolutions-reduce
199,APAC,89.7,,APAC: Augmented PAttern Classification with Neural Networks,/paper/apac-augmented-pattern-classification-with
200,ensemble of 7 models,89.4,,Dynamic Routing Between Capsules,/paper/dynamic-routing-between-capsules
201,DCNN+GFE,89.1,,Deep Convolutional Neural Networks as Generic Feature Extractors,/paper/deep-convolutional-neural-networks-as-generic
202,DCNN,89.0,,ImageNet Classification with Deep Convolutional Neural Networks,/paper/imagenet-classification-with-deep
203,ResNet-14 ,89.0,172000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
204,MCDNN,88.8,,Multi-column Deep Neural Networks for Image Classification,/paper/multi-column-deep-neural-networks-for-image
205,RReLU,88.8,,Empirical Evaluation of Rectified Activations in Convolutional Network,/paper/empirical-evaluation-of-rectified-activations
206,ResNet-56 ,88.8,853000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
207,Diffusion Classifier ,88.5,,Your Diffusion Model is Secretly a Zero-Shot Classifier,/paper/your-diffusion-model-is-secretly-a-zero-shot
208,ReNet,87.7,,ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks,/paper/renet-a-recurrent-neural-network-based
209,TripleNet-B,87.03,12630000.0,Efficient Convolutional Neural Networks on Raspberry Pi for Image Classification,/paper/triplenet-a-low-computing-power-platform-of
210,An Analysis of Unsupervised Pre-training in Light of Recent Advances,86.7,,An Analysis of Unsupervised Pre-training in Light of Recent Advances,/paper/an-analysis-of-unsupervised-pre-training-in
211,ThreshNet95,86.69,16190000.0,ThreshNet: An Efficient DenseNet Using Threshold Mechanism to Reduce Connections,/paper/threshnet-an-efficient-densenet-using
212,ShortNet1-53,86.64,2160000.0,Connection Reduction of DenseNet for Image Recognition,/paper/connection-reduction-is-all-you-need
213,CNN+ Wilson-Cowan model RNN,86.59,,Learning in Wilson-Cowan model for metapopulation,/paper/learning-in-wilson-cowan-model-for
214,ResNet-8 ,86.5,75000.0,Trainable Activations for Image Classification,/paper/trainable-activations-for-image
215,ThresholdNet,86.34,15320000.0,New Pruning Method Based on DenseNet Network for Image Classification,/paper/threshold-pruning-tool-for-densely-connected
216,cvpr_class,85.28,,ResNet strikes back: An improved training procedure in timm,/paper/resnet-strikes-back-an-improved-training
217,WaveMix,85.21,,WaveMix: Multi-Resolution Token Mixing for Images,/paper/wavemix-multi-resolution-token-mixing-for
218,Stochastic Pooling,84.9,,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,/paper/stochastic-pooling-for-regularization-of-deep
219,Improving neural networks by preventing co-adaptation of feature detectors,84.4,,Improving neural networks by preventing co-adaptation of feature detectors,/paper/improving-neural-networks-by-preventing-co
220,CCN,83.36,906075.0,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
221,CvN,83.26,,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
222,CvP,83.19,,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
223,UL-Hopfield ,83.1,,Unsupervised Learning using Pretrained CNN and Associative Memory Bank,/paper/unsupervised-learning-using-pretrained-cnn
224,DCGAN,82.8,,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,/paper/unsupervised-representation-learning-with-1
225,CKN,82.2,,Convolutional Kernel Networks,/paper/convolutional-kernel-networks
226,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,82.0,,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,/paper/discriminative-unsupervised-feature-learning-1
227,Sign-symmetry,80.98,,How Important is Weight Symmetry in Backpropagation?,/paper/how-important-is-weight-symmetry-in
228,pFedBreD_ns_mg,80.63,,Personalized Federated Learning with Hidden Information on Personalized Prior,/paper/personalized-federated-learning-with-hidden
229,1 Layer K-means,80.6,,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,/paper/unsupervised-representation-learning-with-1
230,APVT,80.45,22880000.0,Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions,/paper/aggregated-pyramid-vision-transformer-split
231,Learning with Recursive Perceptual Representations,79.7,,Learning with Recursive Perceptual Representations,/paper/learning-with-recursive-perceptual
232,LeViP,79.5,,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
233,PCANet,78.7,,PCANet: A Simple Deep Learning Baseline for Image Classification?,/paper/pcanet-a-simple-deep-learning-baseline-for
234,Hybrid ViT+RoPE,76.9,,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
235,FLSCNN,75.9,,Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network,/paper/enhanced-image-classification-with-a-fast
236,Hybrid Vision Nystromformer ,75.26,623706.0,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
237,Hybrid PiN,74.0,990298.0,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
238,SmoothNetV1,73.5,,SmoothNets: Optimizing CNN architecture design for differentially private deep learning,/paper/smoothnets-optimizing-cnn-architecture-design
239,SNN,68.3,,Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,/paper/sneaky-spikes-uncovering-stealthy-backdoor
240,Vision Nystromformer ,65.06,530970.0,Vision Xformers: Efficient Attention for Image Classification,/paper/vision-xformers-efficient-attention-for-image
241,ANODE,60.6,,Augmented Neural ODEs,/paper/augmented-neural-odes
242,DTFL,,,Speed Up Federated Learning in Heterogeneous Environment: A Dynamic Tiering Approach,/paper/speed-up-federated-learning-in-heterogeneous
243,ExquisiteNetV2,,518230.0,"A Novel lightweight Convolutional Neural Network, ExquisiteNetV2",/paper/a-novel-lightweight-convolutional-neural
244,kEffNet-B0,,640000.0,Grouped Pointwise Convolutions Significantly Reduces Parameters in EfficientNet,/paper/grouped-pointwise-convolutions-significantly
